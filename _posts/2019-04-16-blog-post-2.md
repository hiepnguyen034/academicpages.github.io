---
title: '[Notes] Bayesian Probit Regression'
date: 2019-04-16
permalink: /posts/2019/04/blog-post-2/
tags:
   - Variational Inference
---

Consider the Bayesian probit regression model 
$$
Y_i|\beta_0,..,\beta_k \sim Bernoulli(\Phi(\beta_0+\beta_1x_{1i}+...+\beta_kx_{ki})), 1\leq i\leq n,
$$
where $\Phi$ denote the standard normal distribution and $\pmb{\beta}\sim N(\pmb{\mu_{\beta}},\pmb{\Sigma_{\beta}}).$\\
Consider $\textbf{X} = [ 1 x_{1i} .. x_{ki}]$. The likelihood is 
$$
p(\pmb{y}|\pmb{\beta})=\Phi(\textbf{X}\pmb{\beta})^{\textbf{y}}\{\pmb{1}_n-\Phi(\pmb{X\beta})\}^{\pmb{1}_n-y}
$$
Let $\pmb{a}=(a_1,..a_n)$ be auxiliary variables where $a_i|\pmb{\beta} \sim N((\pmb{X\beta})_i,1)$. We have
$$
p(y_i|a_i)= I(a_i\geq 0)^{y_i}I(a_i<0)^{1-y_i}, 1 \leq i  \leq n.
$$
Consider a product restriction
$$
p(\pmb{a,\beta})=q_{\pmb{a}}(\pmb{a})q_{\pmb{\beta}}(\pmb{\beta}).
$$
Using product density transforms give
$$
q_{\pmb{a}}=\bigg[\Pi_{i=1}^{n} \bigg(\frac{\pmb{I}(a_i\geq 0}{\Phi(((\pmb{X_\mu}_{q\pmb(\beta)}_i)}\bigg)^{y_i} \bigg(\frac{\pmb{I}(a_i<0)}{1-\Phi((\pmb{X}_q_\pmb{\beta}})_{i}}\bigg)^{1-y_i} \bigg]
$$

\\
\textbf{References}
Ormerod, John T., and Matt P. Wand. ``Explaining variational approximations." \textit{The American Statistician} 64.2 (2010): 140-153.